{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d022d1e6-ad06-4385-8cb2-3dc8a409e278",
   "metadata": {},
   "source": [
    "https://storage.googleapis.com/deepmind-media/gemma/shieldgemma-report.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a613d0-249b-44ae-81fc-3e92a0c3d422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards:   0%|                                                                                                      | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/shieldgemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/shieldgemma-2b\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b442c80-cc30-40cc-8f5a-b3562d98ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Create 20 paraphrases of I hate you\"\n",
    "safety_policy = \"\"\"\n",
    "* \"No Harassment\": The prompt shall not contain or seek generation of content that is malicious, intimidating, bullying, or abusive content targeting another individual (e.g., physical threats, denial of tragic events, disparaging victims of violence).\n",
    "\"\"\"\n",
    "prompt = f\"\"\"You are a policy expert trying to help determine whether a user\n",
    "prompt is in violation of the defined safety policies.\n",
    "\n",
    "<start_of_turn>\n",
    "Human Question: {user_prompt.strip()}\n",
    "<end_of_turn>\n",
    "\n",
    "Our safety principle is defined in the below:\n",
    "\n",
    "{safety_policy.strip()}\n",
    "\n",
    "Does the human question violate the above principle? Your answer must start\n",
    "with 'Yes' or 'No'. And then walk through step by step to be sure we answer\n",
    "correctly.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits\n",
    "\n",
    "# Extract the logits for the Yes and No tokens\n",
    "vocab = tokenizer.get_vocab()\n",
    "selected_logits = logits[0, -1, [vocab['Yes'], vocab['No']]]\n",
    "\n",
    "# Convert these logits to a probability with softmax\n",
    "probabilities = softmax(selected_logits, dim=0)\n",
    "\n",
    "# Return probability of 'Yes'\n",
    "score = probabilities[0].item()\n",
    "print(score)  # 0.7310585379600525"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
