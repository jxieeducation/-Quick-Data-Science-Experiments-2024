{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46759d1-d61e-43aa-979f-d4ecf381c28d",
   "metadata": {},
   "source": [
    "https://docs.wandb.ai/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb147a74-9f17-4fbc-8009-d0b8da49a783",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7b493-c953-47f7-94d9-5522da6369db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Launch 5 simulated experiments\n",
    "total_runs = 5\n",
    "for run in range(total_runs):\n",
    "  # 1️. Start a new run to track this script\n",
    "  wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"basic-intro\",\n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"experiment_{run}\",\n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.02,\n",
    "      \"architecture\": \"CNN\",\n",
    "      \"dataset\": \"CIFAR-100\",\n",
    "      \"epochs\": 10,\n",
    "      })\n",
    "\n",
    "  # This simple block simulates a training loop logging metrics\n",
    "  epochs = 10\n",
    "  offset = random.random() / 5\n",
    "  for epoch in range(2, epochs):\n",
    "      acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "      loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "\n",
    "      # 2️. Log metrics from your script to W&B\n",
    "      wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "  # Mark the run as finished\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512db57-0a0f-412b-8d33-45737a485038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch 3 experiments, trying different dropout rates\n",
    "for _ in range(3):\n",
    "    # initialise a wandb run\n",
    "    wandb.init(\n",
    "        project=\"pytorch-intro\",\n",
    "        config={\n",
    "            \"epochs\": 5,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 1e-3,\n",
    "            \"dropout\": random.uniform(0.01, 0.80),\n",
    "            })\n",
    "\n",
    "    # Copy your config\n",
    "    config = wandb.config\n",
    "\n",
    "    # Get the data\n",
    "    train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)\n",
    "    valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)\n",
    "    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
    "\n",
    "    # A simple MLP model\n",
    "    model = get_model(config.dropout)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "   # Training\n",
    "    example_ct = 0\n",
    "    step_ct = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        for step, (images, labels) in enumerate(train_dl):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            train_loss = loss_func(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            example_ct += len(images)\n",
    "            metrics = {\"train/train_loss\": train_loss,\n",
    "                       \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch,\n",
    "                       \"train/example_ct\": example_ct}\n",
    "\n",
    "            if step + 1 < n_steps_per_epoch:\n",
    "                # Log train metrics to wandb\n",
    "                wandb.log(metrics)\n",
    "\n",
    "            step_ct += 1\n",
    "\n",
    "        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-1)))\n",
    "\n",
    "        # Log train and validation metrics to wandb\n",
    "        val_metrics = {\"val/val_loss\": val_loss,\n",
    "                       \"val/val_accuracy\": accuracy}\n",
    "        wandb.log({**metrics, **val_metrics})\n",
    "\n",
    "        # Save the model checkpoint to wandb\n",
    "        torch.save(model, \"my_model.pt\")\n",
    "        wandb.log_model(\"./my_model.pt\", \"my_mnist_model\", aliases=[f\"epoch-{epoch+1}_dropout-{round(wandb.config.dropout, 4)}\"])\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # If you had a test set, this is how you could log it as a Summary metric\n",
    "    wandb.summary['test_accuracy'] = 0.8\n",
    "\n",
    "    # Close your wandb run\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
