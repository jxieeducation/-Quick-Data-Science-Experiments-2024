{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54675201-2737-4a1d-9d16-54b4d98b2454",
   "metadata": {},
   "source": [
    "Sentence piece tokenizer (XLM-RoBerta tokenizer) https://medium.com/@lars.chr.wiik/how-modern-tokenization-works-d56013a78f1e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0bd8d-5c52-4909-ba5f-569ec85fbb81",
   "metadata": {},
   "source": [
    "* traditionally dividing sentences into words + punctuation based on lang quirks\n",
    "* modern tokenization: language models, embedding models, reranking systems\n",
    "    * speed important for training\n",
    "    * words -> subworks to capture linguistic nuance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5f184-634d-4a85-bbb8-ac58830fc12e",
   "metadata": {},
   "source": [
    "* “Technological advancements have revolutionized communication.”\n",
    "* [“Tech”, ”nolog”, ”ical”, ”advance”, ”ments”, ”have”, ”revol”, ”ution”, ”ized”, ”commun”, ”ica”, ”tion”, “.”].\n",
    "* Popular sub-word algorithms include Byte-pair Encoding (BPE), WordPiece, and SentencePiece."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5cc8c-c1a0-4c4f-b930-0e25daebcfcc",
   "metadata": {},
   "source": [
    "### Byte-pair Encoding\n",
    "* start with individual chars -> merge most frequent adjacent pairs\n",
    "* bytes instead of Unicode characters, allowing the tokenizer to process any input text without pre-defined vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e30e8-a16e-4d7c-96b9-45ebe773406b",
   "metadata": {},
   "source": [
    "### WordPiece Encoding\n",
    "* whole words -> linguistically useful subwords\n",
    "\n",
    "### SentencePiece Encoding\n",
    "* favored when multiple languages simultaneously is important, especially those without clear word boundaries\n",
    "* text as a continuous stream of Unicode characters, not dividing it into discrete words as traditional tokenizers do\n",
    "* It employs either a Byte-Pair Encoding (BPE) method or a Unigram Language Model to analyze this stream.\n",
    "* The unigram model\n",
    "    * begins with a large pool of potential sub-words and iteratively prunes this set based on how likely each sub-word contributes to the likelihood of the text corpus.\n",
    "* SentencePiece may create subwords using parts of adjacent words, without strictly respecting word boundaries.\n",
    "    * particularly beneficial for languages like Japanese or Chinese without clear word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87db47b-13de-4035-aaa7-df470eeca9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
