{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8eb9a1b-7d11-49e9-b01e-9f1e6a13dad8",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/clip-model-and-the-importance-of-multimodal-embeddings-1c8f6b13bf72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce39fbc-71b9-4eff-ab09-73fefb8e9169",
   "metadata": {},
   "source": [
    "* Contrastive Language-Image Pretraining (CLIP)\n",
    "* CLIP’s embeddings for images and text share the same space, enabling direct comparisons between the two modalities\n",
    "* applications of CLIP\n",
    "    * image classification and retrieval: e.g. search image by text descriptions\n",
    "    * content moderation: moderate both text and image in context\n",
    "* process:\n",
    "    * joint training of an image encoder and text encoder\n",
    "    * CLIP loss aims to **maximize the cosine similarity between the image and text embeddings for the N genuine pairs** in the batch while **minimizing the cosine similarity for the N² − N incorrect pairings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4249824-8f72-45ed-8247-a024896870f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "884ff075-e623-45b4-9021-afb6d51be1be",
   "metadata": {},
   "source": [
    "# image_encoder - ResNet or Vision Transformer\n",
    "# text_encoder - CBOW or Text Transformer\n",
    "# I[n, h, w, c] - minibatch of aligned images\n",
    "# T[n, l] - minibatch of aligned texts\n",
    "# W_i[d_i, d_e] - learned proj of image to embed\n",
    "# W_t[d_t, d_e] - learned proj of text to embed\n",
    "# t - learned temperature parameter\n",
    "# extract feature representations of each modality\n",
    "I_f = image_encoder(I) #[n, d_i]\n",
    "T_f = text_encoder(T) #[n, d_t]\n",
    "# joint multimodal embedding [n, d_e]\n",
    "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
    "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
    "# scaled pairwise cosine similarities [n, n]\n",
    "logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
    "# symmetric loss function\n",
    "labels = np.arange(n)\n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
    "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
    "loss = (loss_i + loss_t)/2\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb0c57e1-9812-4571-b147-b8bd4cdf8783",
   "metadata": {},
   "source": [
    "def CLIP_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    n = logits.shape[1]      # number of samples\n",
    "    labels = torch.arange(n) # Create labels tensor\n",
    "    # Calculate cross entropy losses along axis 0 and 1\n",
    "    loss_i = F.cross_entropy(logits.transpose(0, 1), labels, reduction=\"mean\")\n",
    "    loss_t = F.cross_entropy(logits, labels, reduction=\"mean\")\n",
    "    # Calculate the final loss\n",
    "    loss = (loss_i + loss_t) / 2\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666e0e2-296d-4d90-8fbe-bd127193956d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
